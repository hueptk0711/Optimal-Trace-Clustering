{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11026981,"sourceType":"datasetVersion","datasetId":6867024},{"sourceId":12041918,"sourceType":"datasetVersion","datasetId":7577521},{"sourceId":12165608,"sourceType":"datasetVersion","datasetId":7662133},{"sourceId":12165876,"sourceType":"datasetVersion","datasetId":7662318}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip  install pm4py\n!pip install python-Levenshtein\n!pip install scikit-learn\n!pip install paretoset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T00:36:59.090136Z","iopub.execute_input":"2025-06-19T00:36:59.090494Z","iopub.status.idle":"2025-06-19T00:37:14.188935Z","shell.execute_reply.started":"2025-06-19T00:36:59.090475Z","shell.execute_reply":"2025-06-19T00:37:14.188084Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pm4py\nimport cudf\nimport cupy as cp\nimport numpy as np\nfrom sklearn.cluster import DBSCAN as SklearnDBSCAN\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.metrics import silhouette_score, pairwise_distances\nfrom sklearn.manifold import TSNE\nimport seaborn as sns\nfrom pm4py.objects.log.importer.xes import importer as xes_importer\nfrom pm4py.algo.discovery.alpha import algorithm as alpha_miner\nfrom pm4py.algo.conformance.tokenreplay import algorithm as token_replay\nfrom pm4py.algo.evaluation.precision import algorithm as precision_evaluator\nfrom pm4py.algo.evaluation.generalization import algorithm as generalization_evaluator\nfrom pm4py.objects.petri_net.utils import petri_utils\nfrom collections import Counter\nfrom Levenshtein import distance as levenshtein_distance\nimport time\nimport cupyx.scipy.sparse as cusp\nimport concurrent.futures as cf\nimport math\nfrom numba import cuda\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport os\n\ndef setup_environment():\n    \"\"\"Install required packages and set up GPU environment.\"\"\"\n    print(f\"Running: Using {cp.cuda.runtime.getDeviceCount()} GPU(s) with CUDA 12.2...\")\n\ndef load_event_log(file_path):\n    \"\"\"Load and process the event log from an XES file, limiting to the first 1000 traces.\"\"\"\n    print(\"Loading: .xes file...\")\n    start_time = time.time()\n    log = xes_importer.apply(file_path)\n    print(f\"Completed loading .xes file: {time.time() - start_time:.2f}s\")\n    \n    print(\"Converting: Log to list of traces (limited to first 1000 traces)...\")\n    # Limit to the first 1000 traces\n    limited_log = log\n    traces = [tuple(event[\"concept:name\"] for event in trace) for trace in limited_log]\n    unique_activities = sorted(set().union(*[set(trace) for trace in traces]))\n    n_traces = len(traces)\n    print(f\"Completed log conversion: {time.time() - start_time:.2f}s (Number of traces: {n_traces})\")\n    return limited_log, traces, unique_activities, n_traces\n\ndef compute_quality_metrics():\n    \"\"\"Define functions for computing quality metrics.\"\"\"\n    def compute_fitness(net, initial_marking, final_marking, trace_log):\n        replayed_traces = token_replay.apply(trace_log, net, initial_marking, final_marking)\n        fitness = sum(t[\"trace_fitness\"] for t in replayed_traces) / len(replayed_traces)\n        return fitness\n\n    def compute_simplicity(net):\n        num_transitions = len(net.transitions)\n        num_places = len(net.places)\n        return num_transitions + num_places\n\n    def compute_precision(net, initial_marking, final_marking, trace_log):\n        precision = precision_evaluator.apply(\n            trace_log, net, initial_marking, final_marking,\n            variant=precision_evaluator.Variants.ETCONFORMANCE_TOKEN\n        )\n        return precision\n\n    def compute_generalization(net, initial_marking, final_marking, trace_log):\n        generalization = generalization_evaluator.apply(trace_log, net, initial_marking, final_marking)\n        return generalization\n\n    def compute_silhouette_index(data, labels, metric='euclidean', use_gpu=True):\n        print(\"Computing: Running silhouette index...\")\n        start_time = time.time()\n        data = cp.asarray(data) if use_gpu else np.asarray(data)\n        labels = cp.asarray(labels) if use_gpu else np.asarray(labels)\n        \n        unique_labels = cp.unique(labels)\n        n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)\n        \n        if n_clusters < 2:\n            print(\"Warning: Need at least 2 clusters for meaningful Silhouette Index.\")\n            return -1\n        \n        dist_matrix = data if metric == 'precomputed' else pairwise_distances(\n            data.get() if use_gpu else data, metric=metric\n        )\n        dist_matrix = cp.asarray(dist_matrix) if use_gpu else dist_matrix\n        \n        silhouette_scores = []\n        for i in range(len(labels)):\n            if labels[i] == -1:\n                continue\n            same_cluster = (labels == labels[i])\n            a_x = cp.mean(dist_matrix[i, same_cluster]) if cp.sum(same_cluster) > 1 else 0\n            b_x = cp.inf\n            for label in unique_labels:\n                if label == labels[i] or label == -1:\n                    continue\n                other_cluster = (labels == label)\n                if cp.sum(other_cluster) > 0:\n                    b_x = min(b_x, cp.mean(dist_matrix[i, other_cluster]))\n            s_x = 0 if a_x == 0 and b_x == 0 else (b_x - a_x) / max(a_x, b_x)\n            silhouette_scores.append(float(s_x.get() if use_gpu else s_x))\n        \n        silhouette_avg = np.mean(silhouette_scores) if silhouette_scores else -1\n        print(f\"Completed Silhouette Index: {silhouette_avg:.4f} (Time: {time.time() - start_time:.2f}s)\")\n        return silhouette_avg\n    \n    return {\n        \"fitness\": compute_fitness,\n        \"simplicity\": compute_simplicity,\n        \"precision\": compute_precision,\n        \"generalization\": compute_generalization,\n        \"silhouette\": compute_silhouette_index\n    }\n\ndef compute_feature_vectors(traces, unique_activities, n_traces):\n    \"\"\"Compute feature vectors for different methods.\"\"\"\n    def bag_of_activities():\n        print(\"Running: Computing vectors for Bag-of-activities (A1)...\")\n        start_time = time.time()\n        vectors = cp.zeros((n_traces, len(unique_activities)))\n        for i, trace in enumerate(traces):\n            counter = Counter(trace)\n            for j, activity in enumerate(unique_activities):\n                vectors[i, j] = counter[activity]\n        print(f\"Completed A1: {time.time() - start_time:.2f}s\")\n        return vectors, \"A1\"\n\n    def k_gram_model(k=3):\n        print(\"Running: Computing vectors for k-gram model (A2)...\")\n        start_time = time.time()\n        k_grams_set = set()\n        for trace in traces:\n            for i in range(len(trace) - k + 1):\n                k_grams_set.add(trace[i:i+k])\n        k_grams_list = sorted(k_grams_set)\n        vectors = cp.zeros((n_traces, len(k_grams_list)))\n        for i, trace in enumerate(traces):\n            trace_k_grams = Counter([trace[j:j+k] for j in range(len(trace) - k + 1)])\n            for j, k_gram in enumerate(k_grams_list):\n                vectors[i, j] = trace_k_grams[k_gram]\n        print(f\"Completed A2: {time.time() - start_time:.2f}s\")\n        return vectors, \"A2\"\n\n    @cuda.jit\n    def levenshtein_kernel(s_batch, t_batch, distances):\n        i = cuda.grid(1)\n        if i < s_batch.shape[0] * t_batch.shape[0]:\n            s_idx = i // t_batch.shape[0]\n            t_idx = i % t_batch.shape[0]\n            s_len = int(s_batch[s_idx, 0])\n            t_len = int(t_batch[t_idx, 0])\n            s = s_batch[s_idx, 1:s_len+1]\n            t = t_batch[t_idx, 1:t_len+1]\n            dp = cuda.local.array((100, 100), dtype=cp.float32)\n            for m in range(s_len + 1):\n                for n in range(t_len + 1):\n                    if m == 0 and n == 0:\n                        dp[m, n] = 0\n                    elif m == 0:\n                        dp[m, n] = dp[m, n-1] + 1\n                    elif n == 0:\n                        dp[m, n] = dp[m-1, n] + 1\n                    else:\n                        sub_cost = 0 if s[m-1] == t[n-1] else 1\n                        dp[m, n] = min(dp[m-1, n-1] + sub_cost, dp[m-1, n] + 1, dp[m, n-1] + 1)\n            distances[s_idx, t_idx] = dp[s_len, t_len]\n\n    def levenshtein_distance_matrix(chunk_size=10000):\n        print(\"Running: Computing Levenshtein distance matrix on multi-GPU (A3)...\")\n        start_time = time.time()\n        n_chunks = math.ceil(n_traces / chunk_size)\n        max_len = max(max(len(t) for t in traces), 100)\n        act_to_idx = {act: idx for idx, act in enumerate(unique_activities)}\n        traces_padded = cp.zeros((n_traces, max_len + 1), dtype=cp.float32)\n        for i, trace in enumerate(traces):\n            traces_padded[i, 0] = len(trace)\n            for j, act in enumerate(trace):\n                traces_padded[i, j + 1] = act_to_idx[act]\n\n        def process_chunk(i, gpu_id):\n            with cp.cuda.Device(gpu_id):\n                start_idx = i * chunk_size\n                end_idx = min((i + 1) * chunk_size, n_traces)\n                s_batch = traces_padded[start_idx:end_idx]\n                t_batch = traces_padded\n                distances = cp.zeros((s_batch.shape[0], t_batch.shape[0]), dtype=cp.float32)\n                threads_per_block = 256\n                blocks_per_grid = math.ceil((s_batch.shape[0] * t_batch.shape[0]) / threads_per_block)\n                levenshtein_kernel[blocks_per_grid, threads_per_block](\n                    s_batch, t_batch, distances\n                )\n                return distances\n\n        dist_matrix_chunks = []\n        n_gpus = cp.cuda.runtime.getDeviceCount()\n        with cf.ThreadPoolExecutor(max_workers=n_gpus) as executor:\n            futures = [executor.submit(process_chunk, i, i % n_gpus) for i in range(n_chunks)]\n            for idx, future in enumerate(futures):\n                dist_matrix_chunks.append(future.result())\n                progress = (idx + 1) / n_chunks\n                eta = (time.time() - start_time) / progress * (1 - progress)\n                print(f\"A3 Distance Matrix: Processed {idx+1}/{n_chunks} chunks ({progress*100:.2f}%), ETA: {eta:.2f}s\")\n\n        dist_matrix = cp.vstack(dist_matrix_chunks)\n        dist_matrix_full = cp.zeros((n_traces, n_traces), dtype=cp.float32)\n        dist_matrix_full[:dist_matrix.shape[0], :] = dist_matrix\n        dist_matrix_full.T[:dist_matrix.shape[0], :] = dist_matrix\n        dist_matrix_full = cp.maximum(dist_matrix_full, 0)\n        nnz = cusp.csr_matrix(dist_matrix_full).nnz\n        print(f\"Completed A3: {time.time() - start_time:.2f}s\")\n        print(f\"A3 Distance Matrix: Non-zero elements = {nnz}, Total elements = {n_traces * n_traces}\")\n        return dist_matrix_full, \"A3\"\n\n    def get_3grams():\n        activity_to_idx = {a: i for i, a in enumerate(unique_activities)}\n        g3_counts = cp.zeros((len(unique_activities), len(unique_activities), len(unique_activities)), dtype=cp.int32)\n        for trace in traces:\n            for i in range(len(trace) - 2):\n                x, a, y = trace[i:i+3]\n                g3_counts[activity_to_idx[x], activity_to_idx[a], activity_to_idx[y]] += 1\n        return g3_counts\n\n    def compute_substitution_scores():\n        print(\"Running: Computing Substitution Scores (Algorithm 1)...\")\n        start_time = time.time()\n        n_activities = len(unique_activities)\n        g3_counts = get_3grams()\n        Xa = [set() for _ in range(n_activities)]\n        for x_idx in range(n_activities):\n            for a_idx in range(n_activities):\n                for y_idx in range(n_activities):\n                    if g3_counts[x_idx, a_idx, y_idx] > 0:\n                        Xa[a_idx].add((x_idx, y_idx))\n        C = cp.zeros((n_activities, n_activities), dtype=cp.float32)\n        for a_idx in range(n_activities):\n            for b_idx in range(n_activities):\n                if a_idx == b_idx:\n                    counts = g3_counts[:, a_idx, :]\n                    C[a_idx, a_idx] = cp.sum(counts * (counts - 1) / 2)\n                else:\n                    Xab = Xa[a_idx] & Xa[b_idx]\n                    for x_idx, y_idx in Xab:\n                        C[a_idx, b_idx] += g3_counts[x_idx, a_idx, y_idx] * g3_counts[x_idx, b_idx, y_idx]\n        NC = cp.sum(C)\n        M = C / NC\n        pa = cp.zeros(n_activities)\n        for a_idx in range(n_activities):\n            pa[a_idx] = M[a_idx, a_idx] + cp.sum(M[a_idx, :]) - M[a_idx, a_idx]\n        E = cp.zeros((n_activities, n_activities))\n        for a_idx in range(n_activities):\n            for b_idx in range(n_activities):\n                E[a_idx, b_idx] = pa[a_idx]**2 if a_idx == b_idx else 2 * pa[a_idx] * pa[b_idx]\n        S = cp.log2(M / E)\n        S = cp.where(cp.isinf(S) | cp.isnan(S), 0, S)\n        print(f\"Completed Substitution Scores: {time.time() - start_time:.2f}s\")\n        print(f\"sub_scores: min={cp.min(S):.4f}, max={cp.max(S):.4f}, mean={cp.mean(S):.4f}\")\n        return S\n\n    def compute_insertion_scores():\n        print(\"Running: Computing Insertion Scores (Algorithm 2)...\")\n        start_time = time.time()\n        n_activities = len(unique_activities)\n        g3_counts = get_3grams()\n        Xa = [set() for _ in range(n_activities)]\n        for x_idx in range(n_activities):\n            for a_idx in range(n_activities):\n                for y_idx in range(n_activities):\n                    if g3_counts[x_idx, a_idx, y_idx] > 0:\n                        Xa[a_idx].add((x_idx, y_idx))\n        count_right = cp.zeros((n_activities, n_activities), dtype=cp.float32)\n        for a_idx in range(n_activities):\n            for x_idx in range(n_activities):\n                count_right[a_idx, x_idx] = cp.sum(g3_counts[x_idx, a_idx, :])\n        norm = cp.sum(count_right, axis=1)\n        pa = norm / cp.sum(norm)\n        norm_count_right = count_right / norm[:, cp.newaxis]\n        norm_count_right = cp.where(cp.isnan(norm_count_right), 0, norm_count_right)\n        ins_scores = cp.log2(norm_count_right / (pa[:, cp.newaxis] * pa[cp.newaxis, :]))\n        ins_scores = cp.where(cp.isinf(ins_scores) | cp.isnan(ins_scores), 0, ins_scores)\n        print(f\"Completed Insertion Scores: {time.time() - start_time:.2f}s\")\n        print(f\"ins_scores: min={cp.min(ins_scores):.4f}, max={cp.max(ins_scores):.4f}, mean={cp.mean(ins_scores):.4f}\")\n        return ins_scores\n\n    @cuda.jit\n    def edit_distance_kernel(s_batch, t_batch, sub_scores, ins_scores, distances):\n        i = cuda.grid(1)\n        if i < s_batch.shape[0] * t_batch.shape[0]:\n            s_idx = i // t_batch.shape[0]\n            t_idx = i % t_batch.shape[0]\n            s_len = s_batch[s_idx, 0]\n            t_len = t_batch[t_idx, 0]\n            s = s_batch[s_idx, 1:int(s_len+1)]\n            t = t_batch[t_idx, 1:int(t_len+1)]\n            dp = cuda.local.array((100, 100), dtype=cp.float32)\n            for m in range(int(s_len) + 1):\n                for n in range(int(t_len) + 1):\n                    if m == 0 and n == 0:\n                        dp[m, n] = 0\n                    elif m == 0:\n                        dp[m, n] = dp[m, n-1] + max(1.0, abs(ins_scores[int(t[n-1]), int(t[n-1])]))\n                    elif n == 0:\n                        dp[m, n] = dp[m-1, n] + max(1.0, abs(ins_scores[int(s[m-1]), int(s[m-1])]))\n                    else:\n                        sub_cost = 0 if s[m-1] == t[n-1] else max(1.0, abs(sub_scores[int(s[m-1]), int(t[n-1])]))\n                        del_cost = max(1.0, abs(ins_scores[int(s[m-1]), int(s[m-1])]))\n                        ins_cost = max(1.0, abs(ins_scores[int(t[n-1]), int(t[n-1])]))\n                        dp[m, n] = min(dp[m-1, n-1] + sub_cost, dp[m-1, n] + del_cost, dp[m, n-1] + ins_cost)\n            distances[s_idx, t_idx] = dp[int(s_len), int(t_len)]\n\n    def generic_edit_distance_matrix(sub_scores, ins_scores, chunk_size=10000):\n        print(\"Running: Computing Generic Edit Distance matrix on multi-GPU (A4)...\")\n        start_time = time.time()\n        n_chunks = math.ceil(n_traces / chunk_size)\n        max_len = max(max(len(t) for t in traces), 100)\n        act_to_idx = {act: idx for idx, act in enumerate(unique_activities)}\n        traces_padded = cp.zeros((n_traces, max_len + 1), dtype=cp.float32)\n        for i, trace in enumerate(traces):\n            traces_padded[i, 0] = len(trace)\n            for j, act in enumerate(trace):\n                traces_padded[i, j + 1] = act_to_idx[act]\n        sub_scores_gpu = cp.asarray(sub_scores)\n        ins_scores_gpu = cp.asarray(ins_scores)\n\n        def process_chunk(i, gpu_id):\n            with cp.cuda.Device(gpu_id):\n                start_idx = i * chunk_size\n                end_idx = min((i + 1) * chunk_size, n_traces)\n                s_batch = traces_padded[start_idx:end_idx]\n                t_batch = traces_padded\n                distances = cp.zeros((s_batch.shape[0], t_batch.shape[0]), dtype=cp.float32)\n                threads_per_block = 256\n                blocks_per_grid = math.ceil((s_batch.shape[0] * t_batch.shape[0]) / threads_per_block)\n                edit_distance_kernel[blocks_per_grid, threads_per_block](\n                    s_batch, t_batch, sub_scores_gpu, ins_scores_gpu, distances\n                )\n                return distances\n\n        dist_matrix_chunks = []\n        n_gpus = cp.cuda.runtime.getDeviceCount()\n        with cf.ThreadPoolExecutor(max_workers=n_gpus) as executor:\n            futures = [executor.submit(process_chunk, i, i % n_gpus) for i in range(n_chunks)]\n            for idx, future in enumerate(futures):\n                dist_matrix_chunks.append(future.result())\n                progress = (idx + 1) / n_chunks\n                eta = (time.time() - start_time) / progress * (1 - progress)\n                print(f\"A4 Distance Matrix: Processed {idx+1}/{n_chunks} chunks ({progress*100:.2f}%), ETA: {eta:.2f}s\")\n        \n        dist_matrix = cp.vstack(dist_matrix_chunks)\n        dist_matrix_full = cp.zeros((n_traces, n_traces), dtype=cp.float32)\n        dist_matrix_full[:dist_matrix.shape[0], :] = dist_matrix\n        dist_matrix_full.T[:dist_matrix.shape[0], :] = dist_matrix\n        dist_matrix_full = cp.maximum(dist_matrix_full, 0)\n        print(f\"Completed Distance Matrix: {time.time() - start_time:.2f}s\")\n        nnz = cusp.csr_matrix(dist_matrix_full).nnz\n        print(f\"A4 Distance Matrix: Non-zero elements = {nnz}, Total elements = {n_traces * n_traces}\")\n        return dist_matrix_full, \"A4\"\n\n    @cuda.jit\n    def equivalence_kernel(traces_array, counts, R_eq):\n        t_idx, a_idx, b_idx = cuda.grid(3)\n        if t_idx < traces_array.shape[0] and a_idx < R_eq.shape[0] and b_idx < R_eq.shape[1]:\n            if a_idx == b_idx:\n                cuda.atomic.add(R_eq, (a_idx, b_idx), 1)\n            else:\n                trace_len = int(traces_array[t_idx, 0])\n                count_a = 0\n                count_b = 0\n                for i in range(trace_len):\n                    if int(traces_array[t_idx, i + 1]) == a_idx:\n                        count_a += 1\n                    if int(traces_array[t_idx, i + 1]) == b_idx:\n                        count_b += 1\n                if count_a == count_b:\n                    cuda.atomic.add(R_eq, (a_idx, b_idx), 1)\n\n    @cuda.jit\n    def always_after_kernel(traces_array, R_aa):\n        t_idx, a_idx, b_idx = cuda.grid(3)\n        if t_idx < traces_array.shape[0] and a_idx < R_aa.shape[0] and b_idx < R_aa.shape[1] and a_idx != b_idx:\n            trace_len = int(traces_array[t_idx, 0])\n            idx_a = cuda.local.array(100, dtype=cp.int32)\n            idx_b = cuda.local.array(100, dtype=cp.int32)\n            a_count = 0\n            b_count = 0\n            for i in range(trace_len):\n                if int(traces_array[t_idx, i + 1]) == a_idx and a_count < 100:\n                    idx_a[a_count] = i\n                    a_count += 1\n                if int(traces_array[t_idx, i + 1]) == b_idx and b_count < 100:\n                    idx_b[b_count] = i\n                    b_count += 1\n            if a_count > 0 and b_count > 0:\n                if idx_b[0] > idx_a[a_count - 1]:\n                    cuda.atomic.add(R_aa, (a_idx, b_idx), 1)\n\n    @cuda.jit\n    def always_before_kernel(traces_array, R_ab):\n        t_idx, a_idx, b_idx = cuda.grid(3)\n        if t_idx < traces_array.shape[0] and a_idx < R_ab.shape[0] and b_idx < R_ab.shape[1] and a_idx != b_idx:\n            trace_len = int(traces_array[t_idx, 0])\n            idx_a = cuda.local.array(100, dtype=cp.int32)\n            idx_b = cuda.local.array(100, dtype=cp.int32)\n            a_count = 0\n            b_count = 0\n            for i in range(trace_len):\n                if int(traces_array[t_idx, i + 1]) == a_idx and a_count < 100:\n                    idx_a[a_count] = i\n                    a_count += 1\n                if int(traces_array[t_idx, i + 1]) == b_idx and b_count < 100:\n                    idx_b[b_count] = i\n                    b_count += 1\n            if a_count > 0 and b_count > 0:\n                if idx_a[a_count - 1] < idx_b[0]:\n                    cuda.atomic.add(R_ab, (a_idx, b_idx), 1)\n\n    @cuda.jit\n    def never_together_kernel(traces_array, R_nt):\n        t_idx, a_idx, b_idx = cuda.grid(3)\n        if t_idx < traces_array.shape[0] and a_idx < R_nt.shape[0] and b_idx < R_nt.shape[1] and a_idx < b_idx:\n            trace_len = int(traces_array[t_idx, 0])\n            has_a = False\n            has_b = False\n            for i in range(trace_len):\n                if int(traces_array[t_idx, i + 1]) == a_idx:\n                    has_a = True\n                if int(traces_array[t_idx, i + 1]) == b_idx:\n                    has_b = True\n            if not (has_a and has_b):\n                cuda.atomic.add(R_nt, (a_idx, b_idx), 1)\n                cuda.atomic.add(R_nt, (b_idx, a_idx), 1)\n\n    @cuda.jit\n    def directly_follows_kernel(traces_array, R_df):\n        t_idx = cuda.grid(1)\n        if t_idx < traces_array.shape[0]:\n            trace_len = int(traces_array[t_idx, 0])\n            for i in range(trace_len - 1):\n                a_idx = int(traces_array[t_idx, i + 1])\n                b_idx = int(traces_array[t_idx, i + 2])\n                cuda.atomic.add(R_df, (a_idx, b_idx), 1)\n\n    @cuda.jit\n    def log_skeleton_distance_kernel(s_batch, t_batch, R_eq, R_aa, R_ab, R_nt, R_df, C_min, C_max, distances):\n        i = cuda.grid(1)\n        if i < s_batch.shape[0] * t_batch.shape[0]:\n            s_idx = i // t_batch.shape[0]\n            t_idx = i % t_batch.shape[0]\n            s_len = int(s_batch[s_idx, 0])\n            t_len = int(t_batch[t_idx, 0])\n            s = s_batch[s_idx, 1:s_len+1]\n            t = t_batch[t_idx, 1:t_len+1]\n            distance = 0.0\n\n            # R_eq\n            for a_idx in range(R_eq.shape[0]):\n                for b_idx in range(a_idx, R_eq.shape[1]):\n                    if R_eq[a_idx, b_idx]:\n                        count_s_a = 0\n                        count_s_b = 0\n                        count_t_a = 0\n                        count_t_b = 0\n                        for m in range(s_len):\n                            if int(s[m]) == a_idx:\n                                count_s_a += 1\n                            if int(s[m]) == b_idx:\n                                count_s_b += 1\n                        for n in range(t_len):\n                            if int(t[n]) == a_idx:\n                                count_t_a += 1\n                            if int(t[n]) == b_idx:\n                                count_t_b += 1\n                        if (count_s_a == count_s_b) != (count_t_a == count_t_b):\n                            distance += 1.0\n\n            # R_aa and R_ab (combined to reduce memory access)\n            for a_idx in range(R_aa.shape[0]):\n                for b_idx in range(R_aa.shape[1]):\n                    if a_idx == b_idx:\n                        continue\n                    idx_s_a = cuda.local.array(50, dtype=cp.int32)\n                    idx_s_b = cuda.local.array(50, dtype=cp.int32)\n                    idx_t_a = cuda.local.array(50, dtype=cp.int32)\n                    idx_t_b = cuda.local.array(50, dtype=cp.int32)\n                    s_a_count = 0\n                    s_b_count = 0\n                    t_a_count = 0\n                    t_b_count = 0\n                    for m in range(s_len):\n                        if int(s[m]) == a_idx and s_a_count < 50:\n                            idx_s_a[s_a_count] = m\n                            s_a_count += 1\n                        if int(s[m]) == b_idx and s_b_count < 50:\n                            idx_s_b[s_b_count] = m\n                            s_b_count += 1\n                    for n in range(t_len):\n                        if int(t[n]) == a_idx and t_a_count < 50:\n                            idx_t_a[t_a_count] = n\n                            t_a_count += 1\n                        if int(t[n]) == b_idx and t_b_count < 50:\n                            idx_t_b[t_b_count] = n\n                            t_b_count += 1\n                    # R_aa\n                    if R_aa[a_idx, b_idx]:\n                        after_s = s_a_count == 0 or s_b_count == 0 or idx_s_b[0] > idx_s_a[s_a_count - 1]\n                        after_t = t_a_count == 0 or t_b_count == 0 or idx_t_b[0] > idx_t_a[t_a_count - 1]\n                        if after_s != after_t:\n                            distance += 1.0\n                    # R_ab\n                    if R_ab[a_idx, b_idx]:\n                        before_s = s_a_count == 0 or s_b_count == 0 or idx_s_a[s_a_count - 1] < idx_s_b[0]\n                        before_t = t_a_count == 0 or t_b_count == 0 or idx_t_a[t_a_count - 1] < idx_t_b[0]\n                        if before_s != before_t:\n                            distance += 1.0\n\n            # R_nt\n            for a_idx in range(R_nt.shape[0]):\n                for b_idx in range(a_idx + 1, R_nt.shape[1]):\n                    if R_nt[a_idx, b_idx]:\n                        has_s_a = False\n                        has_s_b = False\n                        has_t_a = False\n                        has_t_b = False\n                        for m in range(s_len):\n                            if int(s[m]) == a_idx:\n                                has_s_a = True\n                            if int(s[m]) == b_idx:\n                                has_s_b = True\n                        for n in range(t_len):\n                            if int(t[n]) == a_idx:\n                                has_t_a = True\n                            if int(t[n]) == b_idx:\n                                has_t_b = True\n                        if (has_s_a and has_s_b) != (has_t_a and has_t_b):\n                            distance += 1.0\n\n            # R_df\n            for a_idx in range(R_df.shape[0]):\n                for b_idx in range(R_df.shape[1]):\n                    if R_df[a_idx, b_idx] > 0:\n                        df_s = 0\n                        df_t = 0\n                        for m in range(s_len - 1):\n                            if int(s[m]) == a_idx and int(s[m + 1]) == b_idx:\n                                df_s += 1\n                        for n in range(t_len - 1):\n                            if int(t[n]) == a_idx and int(t[n + 1]) == b_idx:\n                                df_t += 1\n                        distance += abs(df_s - df_t) * 0.1\n\n            # Counters\n            for a_idx in range(len(C_min)):\n                count_s = 0\n                count_t = 0\n                for m in range(s_len):\n                    if int(s[m]) == a_idx:\n                        count_s += 1\n                for n in range(t_len):\n                    if int(t[n]) == a_idx:\n                        count_t += 1\n                if not (C_min[a_idx] <= count_s <= C_max[a_idx] and C_min[a_idx] <= count_t <= C_max[a_idx]):\n                    distance += abs(count_s - count_t) * 0.1\n\n            distances[s_idx, t_idx] = distance\n\n    def log_skeleton_distance_matrix(chunk_size=None):\n        print(\"Running: Computing Log Skeleton distance matrix on multi-GPU (A5)...\")\n        start_time = time.time()\n\n        # Step 1: Preprocess traces\n        alpha, omega = \"α\", \"ω\"\n        extended_activities = [alpha] + unique_activities + [omega]\n        act_to_idx = {act: idx for idx, act in enumerate(extended_activities)}\n        extended_traces = [[alpha] + list(trace) + [omega] for trace in traces]\n        n_activities = len(extended_activities)\n        max_len = max(max(len(t) for t in extended_traces), 100)\n        if max_len > 100:\n            raise ValueError(f\"Trace length {max_len} exceeds maximum supported length (100). Increase kernel array size.\")\n\n        traces_array = cp.zeros((n_traces, max_len + 1), dtype=cp.int32)\n        for i, trace in enumerate(extended_traces):\n            traces_array[i, 0] = len(trace)\n            for j, act in enumerate(trace):\n                traces_array[i, j + 1] = act_to_idx[act]\n\n        # Step 2: Compute relations on GPU\n        print(\"Computing: Log Skeleton relations...\")\n        rel_start_time = time.time()\n\n        # Dynamic chunk size based on GPU memory\n        if chunk_size is None:\n            gpu_mem = cp.cuda.runtime.memGetInfo()[0] // (1024 ** 2)  # Free memory in MB\n            est_mem_per_trace = max_len * 4 / (1024 ** 2)  # Approx memory per trace in MB\n            chunk_size = max(1000, int(gpu_mem * 0.5 / est_mem_per_trace))\n            print(f\"Dynamic chunk_size set to {chunk_size} based on GPU memory ({gpu_mem} MB free)\")\n\n        # Equivalence relation (R_eq)\n        R_eq = cp.zeros((n_activities, n_activities), dtype=cp.int32)\n        threads_per_block = (8, 8, 8)\n        blocks_per_grid = (\n            math.ceil(n_traces / threads_per_block[0]),\n            math.ceil(n_activities / threads_per_block[1]),\n            math.ceil(n_activities / threads_per_block[2])\n        )\n        equivalence_kernel[blocks_per_grid, threads_per_block](traces_array, cp.zeros((n_traces, n_activities), dtype=cp.int32), R_eq)\n        R_eq = (R_eq == n_traces).astype(cp.bool_)\n        R_eq |= R_eq.T\n\n        # Always-after relation (R_aa)\n        R_aa = cp.zeros((n_activities, n_activities), dtype=cp.int32)\n        always_after_kernel[blocks_per_grid, threads_per_block](traces_array, R_aa)\n        R_aa = (R_aa == n_traces).astype(cp.bool_)\n\n        # Always-before relation (R_ab)\n        R_ab = cp.zeros((n_activities, n_activities), dtype=cp.int32)\n        always_before_kernel[blocks_per_grid, threads_per_block](traces_array, R_ab)\n        R_ab = (R_ab == n_traces).astype(cp.bool_)\n\n        # Never-together relation (R_nt)\n        R_nt = cp.zeros((n_activities, n_activities), dtype=cp.int32)\n        never_together_kernel[blocks_per_grid, threads_per_block](traces_array, R_nt)\n        R_nt = (R_nt == n_traces).astype(cp.bool_)\n\n        # Directly-follows relation (R_df)\n        R_df = cp.zeros((n_activities, n_activities), dtype=cp.int32)\n        threads_per_block_df = 256\n        blocks_per_grid_df = math.ceil(n_traces / threads_per_block_df)\n        directly_follows_kernel[blocks_per_grid_df, threads_per_block_df](traces_array, R_df)\n\n        # Counters\n        counts = cp.zeros((n_traces, n_activities), dtype=cp.int32)\n        for i in range(n_traces):\n            trace_len = int(traces_array[i, 0])\n            for j in range(trace_len):\n                act_idx = int(traces_array[i, j + 1])\n                counts[i, act_idx] += 1\n        C_min = cp.min(counts, axis=0)\n        C_max = cp.max(counts, axis=0)\n\n        print(f\"Completed relations computation: {time.time() - rel_start_time:.2f}s\")\n\n        # Step 3: Compute distance matrix on GPU\n        n_chunks = math.ceil(n_traces / chunk_size)\n        traces_padded = traces_array\n\n        def process_chunk(i, gpu_id):\n            with cp.cuda.Device(gpu_id):\n                start_idx = i * chunk_size\n                end_idx = min((i + 1) * chunk_size, n_traces)\n                s_batch = traces_padded[start_idx:end_idx]\n                t_batch = traces_padded\n                distances = cp.zeros((s_batch.shape[0], t_batch.shape[0]), dtype=cp.float32)\n                threads_per_block = 256\n                blocks_per_grid = math.ceil((s_batch.shape[0] * t_batch.shape[0]) / threads_per_block)\n                log_skeleton_distance_kernel[blocks_per_grid, threads_per_block](\n                    s_batch, t_batch, R_eq, R_aa, R_ab, R_nt, R_df, C_min, C_max, distances\n                )\n                return distances\n\n        dist_matrix_chunks = []\n        n_gpus = cp.cuda.runtime.getDeviceCount()\n        with cf.ThreadPoolExecutor(max_workers=n_gpus) as executor:\n            futures = [executor.submit(process_chunk, i, i % n_gpus) for i in range(n_chunks)]\n            for idx, future in enumerate(futures):\n                dist_matrix_chunks.append(future.result())\n                progress = (idx + 1) / n_chunks\n                eta = (time.time() - start_time) / progress * (1 - progress)\n                print(f\"A5 Distance Matrix: Processed {idx+1}/{n_chunks} chunks ({progress*100:.2f}%), ETA: {eta:.2f}s\")\n\n        dist_matrix = cp.vstack(dist_matrix_chunks)\n        dist_matrix_full = cp.zeros((n_traces, n_traces), dtype=cp.float32)\n        dist_matrix_full[:dist_matrix.shape[0], :] = dist_matrix\n        dist_matrix_full.T[:dist_matrix.shape[0], :] = dist_matrix\n        dist_matrix_full = cp.maximum(dist_matrix_full, 0)\n        nnz = cusp.csr_matrix(dist_matrix_full).nnz\n        print(f\"Completed A5: {time.time() - start_time:.2f}s\")\n        print(f\"A5 Distance Matrix: Non-zero elements = {nnz}, Total elements = {n_traces * n_traces}\")\n        return dist_matrix_full, \"A5\"\n\n    sub_scores = compute_substitution_scores()\n    ins_scores = compute_insertion_scores()\n    return [\n        bag_of_activities(),\n        k_gram_model(),\n        levenshtein_distance_matrix(),\n        generic_edit_distance_matrix(sub_scores, ins_scores),\n        log_skeleton_distance_matrix()\n    ]\n\ndef estimate_eps(data):\n    \"\"\"Estimate eps parameter for DBSCAN.\"\"\"\n    if hasattr(data, 'toarray'):\n        dist_array = data.toarray().get()\n    elif hasattr(data, 'get'):\n        dist_array = data.get()\n    else:\n        dist_array = cp.pdist(data).get()\n    positive_dists = dist_array[dist_array > 0]\n    return np.mean(positive_dists) * 0.05 if len(positive_dists) > 0 else 0.1\n\ndef perform_clustering_and_evaluation(log, traces, methods, metrics):\n    \"\"\"Perform clustering and evaluate results.\"\"\"\n    results = {}\n    tsne_data = {}\n    labels_data = {}\n    \n    for method, name in methods:\n        print(f\"Running: Clustering and evaluation for {name}...\")\n        start_time = time.time()\n        \n        eps = estimate_eps(method)\n        print(f\"Estimated eps for {name} (DBSCAN): {eps:.4f}\")\n        \n        if name in [\"A1\", \"A2\"]:\n            clustering_dbscan = SklearnDBSCAN(eps=eps, min_samples=5)\n            labels_dbscan = clustering_dbscan.fit_predict(method.get())\n            data_for_kmeans = method.get()\n        else:\n            dist_matrix = cp.maximum(method, 0)\n            clustering_dbscan = SklearnDBSCAN(eps=eps, min_samples=5, metric=\"precomputed\")\n            labels_dbscan = clustering_dbscan.fit_predict(dist_matrix.get())\n            data_for_kmeans = TSNE(n_components=2, metric=\"precomputed\", init=\"random\", random_state=42).fit_transform(dist_matrix.get())\n        \n        unique_labels_dbscan = np.unique(labels_dbscan)\n        n_clusters_dbscan = len(unique_labels_dbscan) - (1 if -1 in unique_labels_dbscan else 0)\n        print(f\"Number of clusters found for {name} (DBSCAN): {n_clusters_dbscan}\")\n        \n        labels_kmeans = labels_dbscan\n        labels_hierarchical = labels_dbscan\n        if n_clusters_dbscan > 1:\n            kmeans = KMeans(n_clusters=n_clusters_dbscan, random_state=42)\n            labels_kmeans = kmeans.fit_predict(data_for_kmeans)\n            hierarchical = AgglomerativeClustering(n_clusters=n_clusters_dbscan)\n            labels_hierarchical = hierarchical.fit_predict(data_for_kmeans)\n        else:\n            print(f\"Warning: {name} - DBSCAN found {n_clusters_dbscan} clusters, skipping K-means and Hierarchical.\")\n        \n        silhouette_dbscan = metrics[\"silhouette\"](method if name in [\"A1\", \"A2\"] else dist_matrix, \n                                                labels_dbscan, \n                                                metric='euclidean' if name in [\"A1\", \"A2\"] else 'precomputed')\n        silhouette_kmeans = metrics[\"silhouette\"](data_for_kmeans, labels_kmeans, metric='euclidean') if n_clusters_dbscan > 1 else -1\n        silhouette_hierarchical = metrics[\"silhouette\"](data_for_kmeans, labels_hierarchical, metric='euclidean') if n_clusters_dbscan > 1 else -1\n        \n        print(f\"Silhouette Index for {name} - DBSCAN: {silhouette_dbscan:.4f}\")\n        print(f\"Silhouette Index for {name} - K-means: {silhouette_kmeans:.4f}\")\n        print(f\"Silhouette Index for {name} - Hierarchical: {silhouette_hierarchical:.4f}\")\n        \n        tsne = TSNE(n_components=2, random_state=42)\n        tsne_result = tsne.fit_transform(data_for_kmeans)\n        tsne_data[name] = tsne_result\n        labels_data[name] = {\n            \"K-means\": labels_kmeans,\n            \"Hierarchical\": labels_hierarchical,\n            \"DBSCAN\": labels_dbscan\n        }\n        \n        for algo, labels in [(\"DBSCAN\", labels_dbscan), (\"K-means\", labels_kmeans), (\"Hierarchical\", labels_hierarchical)]:\n            print(f\"Running: Clustering with {algo} for {name}...\")\n            unique_labels = np.unique(labels)\n            n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)\n            print(f\"Number of clusters found for {name} ({algo}): {n_clusters}\")\n            \n            clustered_logs = [pm4py.objects.log.obj.EventLog([trace for trace, label in zip(log, labels) if label == i])\n                             for i in unique_labels if i != -1]\n            \n            fitness_scores = []\n            simplicity_scores = []\n            precision_scores = []\n            generalization_scores = []\n            \n            for i, cluster_log in enumerate(clustered_logs):\n                if len(cluster_log) == 0:\n                    continue\n                print(f\"Running: Mining and evaluating cluster {i+1}/{len(clustered_logs)} for {name} ({algo})...\")\n                cluster_start = time.time()\n                net, im, fm = alpha_miner.apply(cluster_log)\n                fitness_scores.append(metrics[\"fitness\"](net, im, fm, cluster_log))\n                simplicity_scores.append(metrics[\"simplicity\"](net))\n                precision_scores.append(metrics[\"precision\"](net, im, fm, cluster_log))\n                generalization_scores.append(metrics[\"generalization\"](net, im, fm, cluster_log))\n                \n                progress = (i + 1) / len(clustered_logs)\n                eta = (time.time() - cluster_start) / progress * (1 - progress)\n                print(f\"Cluster {i+1}/{len(clustered_logs)} completed: {time.time() - cluster_start:.2f}s, ETA: {eta:.2f}s\")\n            \n            results[f\"{name}_{algo}\"] = {\n                \"fitness\": np.mean(fitness_scores) if fitness_scores else 0,\n                \"simplicity\": np.mean(simplicity_scores) if simplicity_scores else 0,\n                \"precision\": np.mean(precision_scores) if precision_scores else 0,\n                \"generalization\": np.mean(generalization_scores) if generalization_scores else 0,\n                \"n_clusters\": n_clusters,\n                \"silhouette\": silhouette_dbscan if algo == \"DBSCAN\" else (silhouette_kmeans if algo == \"K-means\" else silhouette_hierarchical),\n                \"time\": time.time() - start_time\n            }\n    \n    return results, tsne_data, labels_data\n\ndef visualize_results(results, tsne_data, labels_data):\n    \"\"\"Visualize clustering results and display summary table.\"\"\"\n    print(\"Creating: Summary table...\")\n    summary_data = []\n    for method, metrics in results.items():\n        method_name, algo_name = method.split('_')\n        summary_data.append({\n            'Method': method_name,\n            'Algorithm': algo_name,\n            'Fitness': metrics['fitness'],\n            'Simplicity': metrics['simplicity'],\n            'Precision': metrics['precision'],\n            'Generalization': metrics['generalization'],\n            'Number of Clusters': metrics['n_clusters'],\n            'Silhouette Index': metrics['silhouette'],\n            'Time (s)': metrics['time']\n        })\n    \n    df = pd.DataFrame(summary_data)\n    df = df.round(4)\n    df = df.sort_values(by=['Method', 'Algorithm'])\n    \n    print(\"\\nSummary of Clustering Results:\")\n    print(df.to_string(index=False))\n    \n    df.to_csv('BPI_Challenge_2017.csv', index=False)\n    print(\"Summary table saved to 'clustering_results.csv'\")\n    \n    print(\"\\nRunning: Printing detailed results...\")\n    for method, metrics in results.items():\n        print(f\"{method}:\")\n        print(f\"  Fitness: {metrics['fitness']:.2f}\")\n        print(f\"  Simplicity: {metrics['simplicity']:.2f}\")\n        print(f\"  Precision: {metrics['precision']:.2f}\")\n        print(f\"  Generalization: {metrics['generalization']:.2f}\")\n        print(f\"  Number of Clusters: {metrics['n_clusters']}\")\n        print(f\"  Silhouette Index: {metrics['silhouette']:.4f}\")\n        print(\"    => \" + (\n            \"Good clustering quality (Silhouette > 0.5)\" if metrics['silhouette'] > 0.5 else\n            \"Acceptable clustering quality (0 < Silhouette ≤ 0.5)\" if metrics['silhouette'] > 0 else\n            \"Poor clustering quality (Silhouette ≤ 0)\"\n        ))\n        print(f\"  Time: {metrics['time']:.2f}s\")\n\ndef main(file_path):\n    \"\"\"Main function to run the entire pipeline.\"\"\"\n    setup_environment()\n    log, traces, unique_activities, n_traces = load_event_log(file_path)\n    metrics = compute_quality_metrics()\n    methods = compute_feature_vectors(traces, unique_activities, n_traces)\n    results, tsne_data, labels_data = perform_clustering_and_evaluation(log, traces, methods, metrics)\n    visualize_results(results, tsne_data, labels_data)\n    print(\"Completed!\")\n\nif __name__ == \"__main__\":\n    input_file = \"/kaggle/input/event-log/RequestForPayment.xes\"\n    main(input_file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T00:37:14.193453Z","iopub.execute_input":"2025-06-19T00:37:14.193630Z","execution_failed":"2025-06-19T00:38:42.372Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport sklearn\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import classification_report\nfrom sklearn.cluster import KMeans\nfrom paretoset import paretoset\nimport warnings\nimport os\n\nwarnings.filterwarnings('ignore')  # Suppress warnings for cleaner output\n\n# Disable cudf.pandas acceleration\nos.environ[\"CUDF_PANDAS_ACCELERATE\"] = \"false\"  # Prevent cudf.pandas acceleration\n\n# Print package versions for debugging\nprint(\"Package Versions:\")\nprint(f\"pandas: {pd.__version__}\")\nprint(f\"scikit-learn: {sklearn.__version__}\")\nprint(f\"numpy: {np.__version__}\")\n\n# Step 1: Load the results\ntry:\n    df = pd.read_csv('BPI_Challenge_2017.csv')\nexcept FileNotFoundError:\n    print(\"Error: 'BPI_Challenge_2017.csv' not found. Ensure the file exists in the working directory.\")\n    exit(1)\n\n# Convert to standard pandas DataFrame\ndf = pd.DataFrame(df)\n\n# Inspect data for issues\nprint(\"\\nData Info:\")\nprint(df.info())\nprint(\"\\nFirst 5 rows of data:\")\nprint(df.head())\nprint(\"\\nChecking for NaN or infinite values:\")\nprint(df[['Fitness', 'Simplicity', 'Precision', 'Generalization']].isna().sum())\nprint(df[['Fitness', 'Simplicity', 'Precision', 'Generalization']].apply(lambda x: np.isinf(x).sum()))\n\n# Step 2: Automatically determine thresholds for Quality classes\ndef assign_quality_dynamic(silhouette, thresholds):\n    if silhouette >= thresholds['good']:\n        return 'Good'\n    elif silhouette >= thresholds['acceptable']:\n        return 'Acceptable'\n    else:\n        return 'Poor'\n\n# Compute dynamic thresholds using K-means clustering on Silhouette Index\ndef compute_dynamic_thresholds(silhouette_values):\n    silhouette_values = np.array(silhouette_values).reshape(-1, 1)\n    if len(np.unique(silhouette_values)) < 3:\n        print(\"Warning: Insufficient unique Silhouette Index values for clustering. Using quantile-based thresholds.\")\n        return {\n            'good': np.percentile(silhouette_values, 75),  # Top 25%\n            'acceptable': np.percentile(silhouette_values, 25)  # Bottom 25%\n        }\n    kmeans = KMeans(n_clusters=3, random_state=42)\n    kmeans.fit(silhouette_values)\n    cluster_centers = np.sort(kmeans.cluster_centers_.flatten())\n    return {\n        'good': cluster_centers[2],  # Highest cluster center\n        'acceptable': cluster_centers[1]  # Middle cluster center\n    }\n\n# Verify Silhouette Index column exists\nif 'Silhouette Index' not in df.columns:\n    print(\"Error: 'Silhouette Index' column missing in CSV file.\")\n    exit(1)\n\n# Compute thresholds and assign Quality\nthresholds = compute_dynamic_thresholds(df['Silhouette Index'])\nprint(f\"\\nDynamic Thresholds: Good >= {thresholds['good']:.4f}, Acceptable >= {thresholds['acceptable']:.4f}\")\ndf['Quality'] = df['Silhouette Index'].apply(lambda x: assign_quality_dynamic(x, thresholds))\nprint(\"\\nQuality Distribution:\")\nprint(df['Quality'].value_counts())\n\n# Step 3: Compute composite score for tie-breaking in Pareto Front\ndef compute_composite_score(row, weights=None):\n    if weights is None:\n        weights = {'Silhouette Index': 0.4, 'Fitness': 0.3, 'Precision': 0.2, 'Generalization': 0.1}\n    # Normalize metrics to [0, 1]\n    normalized = {}\n    for metric in ['Silhouette Index', 'Fitness', 'Precision', 'Generalization']:\n        values = df[metric]\n        min_val, max_val = values.min(), values.max()\n        normalized[metric] = (row[metric] - min_val) / (max_val - min_val + 1e-10)  # Avoid division by zero\n    score = sum(normalized[metric] * weights[metric] for metric in weights)\n    return score\n\ndf['Combination'] = df['Method'] + '_' + df['Algorithm']\ndf['Composite_Score'] = df.apply(compute_composite_score, axis=1)\n\n# Step 4: Select Pareto Front and best combination\ndef select_best_combination(df, thresholds):\n    # Filter combinations above acceptable threshold\n    good_combinations = df[df['Silhouette Index'] >= thresholds['acceptable']]\n    \n    if len(good_combinations) == 0:\n        print(\"Warning: No combinations above acceptable threshold. Selecting best by Composite Score.\")\n        best_combination_idx = df['Composite_Score'].idxmax()\n        return best_combination_idx\n    \n    # Compute Pareto Front\n    metrics = ['Silhouette Index', 'Fitness', 'Precision', 'Generalization']\n    pareto_mask = paretoset(good_combinations[metrics], sense=['max', 'max', 'max', 'max'])\n    pareto_combinations = good_combinations[pareto_mask]\n    \n    print(f\"\\nPareto Front: {len(pareto_combinations)} combinations\")\n    print(pareto_combinations[['Combination', 'Silhouette Index', 'Fitness', 'Precision', 'Generalization', 'Composite_Score']])\n    \n    if len(pareto_combinations) > 1:\n        # Select second-best by Composite Score\n        pareto_combinations = pareto_combinations.sort_values(by='Composite_Score', ascending=False)\n        best_combination_idx = pareto_combinations.index[1]  # Second-best\n    else:\n        best_combination_idx = pareto_combinations.index[0]\n    \n    return best_combination_idx\n\n# Step 5: Check for single class and select best combination if necessary\nif len(df['Quality'].unique()) < 2:\n    print(\"Warning: Only one class found in 'Quality'. Skipping classification and selecting best combination by Pareto Front.\")\n    best_combination_idx = select_best_combination(df, thresholds)\n    \n    best_combination = df.loc[best_combination_idx, 'Combination']\n    best_metrics = df.loc[best_combination_idx, ['Fitness', 'Simplicity', 'Precision', 'Generalization']]\n    best_silhouette = df.loc[best_combination_idx, 'Silhouette Index']\n    best_composite_score = df.loc[best_combination_idx, 'Composite_Score']\n    \n    print(\"\\nBest Combination (No Classification):\")\n    print(f\"Combination: {best_combination}\")\n    print(f\"Quality: {df.loc[best_combination_idx, 'Quality']}\")\n    print(f\"Metrics: Fitness={best_metrics['Fitness']:.4f}, Simplicity={best_metrics['Simplicity']:.4f}, \"\n          f\"Precision={best_metrics['Precision']:.4f}, Generalization={best_metrics['Generalization']:.4f}\")\n    print(f\"Silhouette Index: {best_silhouette:.4f}\")\n    print(f\"Composite Score: {best_composite_score:.4f}\")\n    exit(0)\n\n# Step 6: Preprocess for classification\n# Encode categorical variables\nle_method = LabelEncoder()\nle_algorithm = LabelEncoder()\ndf['Method_Encoded'] = le_method.fit_transform(df['Method'])\ndf['Algorithm_Encoded'] = le_algorithm.fit_transform(df['Algorithm'])\n\n# Select features and target\nfeatures = ['Fitness', 'Simplicity', 'Precision', 'Generalization']\nif not all(col in df.columns for col in features):\n    print(f\"Error: One or more feature columns {features} missing in CSV file.\")\n    exit(1)\n\nX = df[features].to_numpy()\ny = df['Quality'].to_numpy()\n\n# Check for NaN or infinite values\nif np.any(np.isnan(X)) or np.any(np.isinf(X)):\n    print(\"Warning: Found NaN or infinite values in features. Replacing with 0.\")\n    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n\n# Standardize features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Step 7: Split data\ntry:\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_scaled, y, test_size=0.2, random_state=42, stratify=y\n    )\nexcept ValueError as e:\n    print(f\"Error during train-test split: {e}\")\n    print(\"Class distribution in full dataset:\")\n    print(pd.Series(y).value_counts())\n    print(\"Falling back to non-stratified split...\")\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_scaled, y, test_size=0.2, random_state=42\n    )\n    if len(np.unique(y_train)) < 2:\n        print(\"Error: Training set contains only one class after non-stratified split.\")\n        print(\"Training set class distribution:\")\n        print(pd.Series(y_train).value_counts())\n        print(\"Selecting best combination by Pareto Front.\")\n        best_combination_idx = select_best_combination(df, thresholds)\n        \n        best_combination = df.loc[best_combination_idx, 'Combination']\n        best_metrics = df.loc[best_combination_idx, ['Fitness', 'Simplicity', 'Precision', 'Generalization']]\n        best_silhouette = df.loc[best_combination_idx, 'Silhouette Index']\n        best_composite_score = df.loc[best_combination_idx, 'Composite_Score']\n        \n        print(\"\\nBest Combination (No Classification):\")\n        print(f\"Combination: {best_combination}\")\n        print(f\"Quality: {df.loc[best_combination_idx, 'Quality']}\")\n        print(f\"Metrics: Fitness={best_metrics['Fitness']:.4f}, Simplicity={best_metrics['Simplicity']:.4f}, \"\n              f\"Precision={best_metrics['Precision']:.4f}, Generalization={best_metrics['Generalization']:.4f}\")\n        print(f\"Silhouette Index: {best_silhouette:.4f}\")\n        print(f\"Composite Score: {best_composite_score:.4f}\")\n        exit(0)\n\nprint(f\"\\nTraining set size: {X_train.shape[0]} samples\")\nprint(f\"Test set size: {X_test.shape[0]} samples\")\nprint(\"Training set class distribution:\")\nprint(pd.Series(y_train).value_counts())\n\n# Step 8: Define and fine-tune the model\nparam_grid = {\n    'n_estimators': [50, 100],\n    'max_depth': [3, 5],\n    'learning_rate': [0.01, 0.1]\n}\ngb = GradientBoostingClassifier(random_state=42)\ncv = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\ngrid_search = GridSearchCV(\n    gb, param_grid, cv=cv, scoring='f1_weighted', n_jobs=1, error_score='raise'\n)\n\ntry:\n    grid_search.fit(X_train, y_train)\n    best_model = grid_search.best_estimator_\n    print(f\"Best parameters: {grid_search.best_params_}\")\nexcept Exception as e:\n    print(f\"Error during GridSearchCV fit: {e}\")\n    print(\"Falling back to default GradientBoostingClassifier...\")\n    try:\n        gb = GradientBoostingClassifier(random_state=42)\n        gb.fit(X_train, y_train)\n        best_model = gb\n    except ValueError as ve:\n        print(f\"Error during fallback fit: {ve}\")\n        print(\"Training set class distribution:\")\n        print(pd.Series(y_train).value_counts())\n        print(\"Selecting best combination by Pareto Front.\")\n        best_combination_idx = select_best_combination(df, thresholds)\n        \n        best_combination = df.loc[best_combination_idx, 'Combination']\n        best_metrics = df.loc[best_combination_idx, ['Fitness', 'Simplicity', 'Precision', 'Generalization']]\n        best_silhouette = df.loc[best_combination_idx, 'Silhouette Index']\n        best_composite_score = df.loc[best_combination_idx, 'Composite_Score']\n        \n        print(\"\\nBest Combination (No Classification):\")\n        print(f\"Combination: {best_combination}\")\n        print(f\"Quality: {df.loc[best_combination_idx, 'Quality']}\")\n        print(f\"Metrics: Fitness={best_metrics['Fitness']:.4f}, Simplicity={best_metrics['Simplicity']:.4f}, \"\n              f\"Precision={best_metrics['Precision']:.4f}, Generalization={best_metrics['Generalization']:.4f}\")\n        print(f\"Silhouette Index: {best_silhouette:.4f}\")\n        print(f\"Composite Score: {best_composite_score:.4f}\")\n        exit(0)\n\n# Step 9: Evaluate the model\ny_pred = best_model.predict(X_test)\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred, zero_division=0))\n\n# Step 10: Predict the best combination\ndf['Predicted_Quality'] = best_model.predict(scaler.transform(df[features].to_numpy()))\ndf['Combination'] = df['Method'] + '_' + df['Algorithm']\n\n# Select the best combination using Pareto Front\nquality_priority = {'Good': 3, 'Acceptable': 2, 'Poor': 1}\ndf['Quality_Score'] = df['Predicted_Quality'].map(quality_priority)\n\nbest_combination_idx = select_best_combination(df, thresholds)\nbest_combination = df.loc[best_combination_idx, 'Combination']\nbest_metrics = df.loc[best_combination_idx, features]\nbest_silhouette = df.loc[best_combination_idx, 'Silhouette Index']\nbest_composite_score = df.loc[best_combination_idx, 'Composite_Score']\n\nprint(\"\\nBest Combination:\")\nprint(f\"Combination: {best_combination}\")\nprint(f\"Predicted Quality: {df.loc[best_combination_idx, 'Predicted_Quality']}\")\nprint(f\"Metrics: Fitness={best_metrics['Fitness']:.4f}, Simplicity={best_metrics['Simplicity']:.4f}, \"\n      f\"Precision={best_metrics['Precision']:.4f}, Generalization={best_metrics['Generalization']:.4f}\")\nprint(f\"Silhouette Index: {best_silhouette:.4f}\")\nprint(f\"Composite Score: {best_composite_score:.4f}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-19T00:38:42.372Z"}},"outputs":[],"execution_count":null}]}